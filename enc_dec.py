# -*- coding: utf-8 -*-
"""Enc_Dec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QPqpBl1yomjUGtaN-Ogv57hF66cSwVjf
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import torch.nn.functional as F
import numpy as np

'''# Instantiates the device to be used as GPU/CPU based on availability
device_gpu = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Visualization tools
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import clear_output

import random'''

eng_alphabets = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
pad_char = '-PAD-'

eng_alpha2index = {pad_char: 0}
for index, alpha in enumerate(eng_alphabets):
    eng_alpha2index[alpha] = index+1

print(eng_alpha2index)

eng_alpha2index[pad_char]

hindi_alphabets = [chr(alpha) for alpha in range(2304,2432)]
hindi_alphabet_size = len(hindi_alphabets)

hindi_alpha2index = {pad_char: 0}
for index, alpha in enumerate(hindi_alphabets):
    hindi_alpha2index[alpha] = index+1

'''print(hindi_alpha2index)

import re
non_engletters = re.compile('[^a-zA-Z ]')

def clean_english(line):
  line = non_engletters.sub(' ', line)
  line = line.upper()
  return line.split()
def clean_hindi(line):
  line = line.replace('-', ' ').replace(',', ' ')
  cleaned_line = ''
  for char in line:
    if char in hindi_alpha2index or char == ' ':
      cleaned_line += char
  return cleaned_line.split()

from torch.utils.data import Dataset
import xml.etree.ElementTree as ET

class TransliterationDataLoader(Dataset):
    def __init__(self, filename):
        self.eng_words, self.hindi_words = self.readXmlDataset(filename, clean_hindi)
        self.shuffle_indices = list(range(len(self.eng_words)))
        random.shuffle(self.shuffle_indices)
        self.shuffle_start_index = 0
        
    def __len__(self):
        return len(self.eng_words)
    
    def __getitem__(self, idx):
        return self.eng_words[idx], self.hindi_words[idx]
    
    def readXmlDataset(self, filename, lang_vocab_cleaner):
        transliterationCorpus = ET.parse(filename).getroot()
        lang1_words = []
        lang2_words = []

        for line in transliterationCorpus:
            wordlist1 = clean_english(line[0].text)
            wordlist2 = lang_vocab_cleaner(line[1].text)

            # Skip noisy data
            if len(wordlist1) != len(wordlist2):
                print('Skipping: ', line[0].text, ' - ', line[1].text)
                continue

            for word in wordlist1:
                lang1_words.append(word)
            for word in wordlist2:
                lang2_words.append(word)

        return lang1_words, lang2_words
    
    def get_random_sample(self):
        return self.__getitem__(np.random.randint(len(self.eng_words)))
    
    def get_batch_from_array(self, batch_size, array):
        end = self.shuffle_start_index + batch_size
        batch = []
        if end >= len(self.eng_words):
            batch = [array[i] for i in self.shuffle_indices[0:end%len(self.eng_words)]]
            end = len(self.eng_words)
        return batch + [array[i] for i in self.shuffle_indices[self.shuffle_start_index : end]]
    
    def get_batch(self, batch_size, postprocess = True):
        eng_batch = self.get_batch_from_array(batch_size, self.eng_words)
        hindi_batch = self.get_batch_from_array(batch_size, self.hindi_words)
        self.shuffle_start_index += batch_size + 1
        
        # Reshuffle if 1 epoch is complete
        if self.shuffle_start_index >= len(self.eng_words):
            random.shuffle(self.shuffle_indices)
            self.shuffle_start_index = 0
            
        return eng_batch, hindi_batch

training_data = TransliterationDataLoader('NEWS2012-Training-EnHi-13937.xml')
testing_data = TransliterationDataLoader('NEWS2012-Testing-EnHi-1000.xml')

print(len(training_data),len(testing_data))

for i in range(10):
  index = np.random.randint(len(training_data))
  eng,hindi = training_data[index]
  print(eng,':',hindi)'''

def eng_word_rep(word,letters):
  rep = torch.zeros(len(word)+1,1,len(letters))
  for let_ind,let in enumerate(word):
    pos = letters[let]
    rep[let_ind][0][pos] = 1
  pad_pos = letters[pad_char]
  rep[let_ind + 1][0][pad_pos] = 1
  return rep

def hindi_word_rep(word,letters):
  rep = torch.zeros([len(word)+1,1], dtype = torch.long)
  for let_ind,let in enumerate(word):
    pos = letters[let]
    rep[let_ind][0] = pos
  pad_pos = letters[pad_char]
  rep[let_ind + 1][0] = pad_pos
  return rep

'''eng,hin = training_data[30]
print(eng)
rep = eng_word_rep(eng,eng_alpha2index)
print(rep)
h_rep = hindi_word_rep(hin,hindi_alpha2index)
print(hin,h_rep)'''

MAX_OUTPUT_CHARS = 30
class Transliteration_EncoderDecoder(nn.Module):
    
    def __init__(self, input_size, hidden_size, output_size, verbose=False):
        super(Transliteration_EncoderDecoder, self).__init__()
        
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        self.encoder_rnn_cell = nn.GRU(input_size, hidden_size)
        self.decoder_rnn_cell = nn.GRU(output_size, hidden_size)
        
        self.h2o = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=2)
        
        self.verbose = verbose
        
    def forward(self, input, max_output_chars = MAX_OUTPUT_CHARS, device = 'cpu', ground_truth = None):
        
        # encoder
        out, hidden = self.encoder_rnn_cell(input)
        
        if self.verbose:
            print('Encoder input', input.shape)
            print('Encoder output', out.shape)
            print('Encoder hidden', hidden.shape)
        
        # decoder
        decoder_state = hidden
        decoder_input = torch.zeros(1, 1, self.output_size).to(device)
        outputs = []
        
        if self.verbose:
            print('Decoder state', decoder_state.shape)
            print('Decoder input', decoder_input.shape)
        
        for i in range(max_output_chars):
            
            out, decoder_state = self.decoder_rnn_cell(decoder_input, decoder_state)
            
            if self.verbose:
                print('Decoder intermediate output', out.shape)
            
            out = self.h2o(decoder_state)
            out = self.softmax(out)
            outputs.append(out.view(1, -1))
            
            if self.verbose:
                print('Decoder output', out.shape)
                self.verbose = False
            
            max_idx = torch.argmax(out, 2, keepdim=True)
            if not ground_truth is None:
                max_idx = ground_truth[i].reshape(1, 1, 1)
            one_hot = torch.FloatTensor(out.shape).to(device)
            one_hot.zero_()
            one_hot.scatter_(2, max_idx, 1)
            
            decoder_input = one_hot.detach()
            
        return outputs

class Transliteration_EncoderDecoder_Attention(nn.Module):
    
    def __init__(self, input_size, hidden_size, output_size, verbose=False):
        super(Transliteration_EncoderDecoder_Attention, self).__init__()
        
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        self.encoder_rnn_cell = nn.GRU(input_size, hidden_size)
        self.decoder_rnn_cell = nn.GRU(hidden_size*2, hidden_size)
        
        self.h2o = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=2)
        
        self.U = nn.Linear(self.hidden_size, self.hidden_size)
        self.W = nn.Linear(self.hidden_size, self.hidden_size)
        self.attn = nn.Linear(self.hidden_size, 1)
        self.out2hidden = nn.Linear(self.output_size, self.hidden_size)   
        
        self.verbose = verbose
        
    def forward(self, input, max_output_chars = MAX_OUTPUT_CHARS, device = 'cpu', ground_truth = None):
        
        # encoder
        encoder_outputs, hidden = self.encoder_rnn_cell(input)
        encoder_outputs = encoder_outputs.view(-1, self.hidden_size)
        
        if self.verbose:
            print('Encoder output', encoder_outputs.shape)
        
        # decoder
        decoder_state = hidden
        decoder_input = torch.zeros(1, 1, self.output_size).to(device)
        
        outputs = []
        U = self.U(encoder_outputs)
        
        if self.verbose:
            print('Decoder state', decoder_state.shape)
            print('Decoder intermediate input', decoder_input.shape)
            print('U * Encoder output', U.shape)
        
        for i in range(max_output_chars):
            
            W = self.W(decoder_state.view(1, -1).repeat(encoder_outputs.shape[0], 1))
            V = self.attn(torch.tanh(U + W))
            attn_weights = F.softmax(V.view(1, -1), dim = 1) 
            
            if self.verbose:
                print('W * Decoder state', W.shape)
                print('V', V.shape)
                print('Attn', attn_weights.shape)
            
            attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 encoder_outputs.unsqueeze(0))
            
            embedding = self.out2hidden(decoder_input)
            decoder_input = torch.cat((embedding[0], attn_applied[0]), 1).unsqueeze(0)
            
            if self.verbose:
                print('Attn LC', attn_applied.shape)
                print('Decoder input', decoder_input.shape)
                
            out, decoder_state = self.decoder_rnn_cell(decoder_input, decoder_state)
            
            if self.verbose:
                print('Decoder intermediate output', out.shape)
                
            out = self.h2o(decoder_state)
            out = self.softmax(out)
            outputs.append(out.view(1, -1))
            
            if self.verbose:
                print('Decoder output', out.shape)
                self.verbose = False
            
            max_idx = torch.argmax(out, 2, keepdim=True)
            if not ground_truth is None:
                max_idx = ground_truth[i].reshape(1, 1, 1)
            one_hot = torch.zeros(out.shape, device=device)
            one_hot.scatter_(2, max_idx, 1) 
            
            decoder_input = one_hot.detach()
            
        return outputs
    def infer(self,word,net,device = 'cpu'):
        str1=''
        word = str(word)
        max_chars = len(word)
        word = word.upper()
        word_rep = eng_word_rep(word,eng_alpha2index)
        out = net.forward(word_rep,max_chars)
        for k in range(max_chars):
            k = torch.argmax(out[k],1).data
            str1 = str1 + str(list(hindi_alpha2index.keys())[list(hindi_alpha2index.values()).index(k)])
        return str1

'''net_attn = Transliteration_EncoderDecoder_Attention(len(eng_alpha2index), 256, len(hindi_alpha2index), verbose=True)

def train_batch(net, opt, criterion, batch_size, device = 'cpu', teacher_force = False):
    
    net.train().to(device)
    opt.zero_grad()
    eng_batch, hindi_batch = training_data.get_batch(batch_size)
    
    total_loss = 0
    for i in range(batch_size):
        
        input = eng_word_rep(eng_batch[i], eng_alpha2index, device)
        gt = hindi_word_rep(hindi_batch[i], hindi_alpha2index, device)
        outputs = net(input, gt.shape[0], device, ground_truth = gt if teacher_force else None)
        
        for index, output in enumerate(outputs):
            loss = criterion(output, gt[index]) / batch_size
            loss.backward(retain_graph = True)
            total_loss += loss
        
    opt.step()
    return total_loss/batch_size

def train_setup(net, lr = 0.01, n_batches = 100, batch_size = 10, momentum = 0.9, display_freq=5, device = 'cpu'):
    
    net = net.to(device)
    criterion = nn.NLLLoss(ignore_index = -1)
    opt = optim.Adam(net.parameters(), lr=lr)
    teacher_force_upto = n_batches//3
    
    loss_arr = np.zeros(n_batches + 1)
    
    for i in range(n_batches):
        loss_arr[i+1] = (loss_arr[i]*i + train_batch(net, opt, criterion, batch_size, device = device, teacher_force = i<teacher_force_upto ))/(i + 1)
        
        if i%display_freq == display_freq-1:
            clear_output(wait=True)
            
            print('Iteration', i, 'Loss', loss_arr[i])
            plt.figure()
            plt.plot(loss_arr[1:i], '-*')
            plt.xlabel('Iteration')
            plt.ylabel('Loss')
            plt.show()
            print('\n\n')
            
    torch.save(net, 'model.pt')
    return loss_arr

net = Transliteration_EncoderDecoder(len(eng_alpha2index), 256, len(hindi_alpha2index))

device_gpu

train_setup(net_attn,lr = 0.01,n_batches=700,batch_size=32,display_freq = 50,device=device_gpu)

def infer(net,word,max_chars,device = 'cpu'):
  net.eval().to(device)
  str1=''
  word_rep = eng_word_rep(word,eng_alpha2index,device)
  out = net.forward(word_rep,max_chars)
  for k in range(max_chars):
    k = torch.argmax(out[k],1).data
    str1 = str1 + str(list(hindi_alpha2index.keys())[list(hindi_alpha2index.values()).index(k)])
  print(str1)

infer(net_attn,'TEJA',3)
#val = out[0]
#print(out)
#ind,value = val.topk(1)
#value.shape
#val = value.tolist()[0]
#print(val[0])

print(hindi_alpha2index[val[0]])

import pickle

pickle.dump(net_attn,open('eng2hin.pkl','wb'))

eng,hin= training_data[15]
print(hin)
k =hindi_word_rep(hin,hindi_alpha2index)
print(k,k.shape)

str1 = ''
for k in range(4):
  k = torch.argmax(out[k],1).data
  str1 = str1 + str(list(hindi_alpha2index.keys())[list(hindi_alpha2index.values()).index(k)])

print(str1)

k = torch.randn(5)
val,ind = k.topk(1)
print(ind.tolist()[0][0])

def evaluate(net,device = 'cpu'):
  net = net.eval()
  accuracy = 0
  for i in range(len(testing_data)):
    eng,hin = testing_data[i]
    hin_rep = hindi_word_rep(hin,hindi_alpha2index)
    output = infer(net,eng,hin_rep.shape[0],device)
    correct  = 0
    for index,value in enumerate(output):
      val,ind = value.topk(1)
      hindi_pos = ind.tolist()[0]
      if hindi_pos[0] == hin_rep[index][0]:
        correct+=1
    accuracy += correct/hin_rep.shape[0]
  accuracy = accuracy/len(testing_data)
  print(accuracy)

evaluate(net)
'''
